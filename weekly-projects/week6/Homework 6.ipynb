{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e918709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homework 6 (due 08/08/2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b444be24-24ae-496b-92a7-5cf8abc38e05",
   "metadata": {},
   "source": [
    "# Neural networks and computer vision\n",
    "\n",
    "### Objective\n",
    "In this week's project, you will learn to train, validate, and test a neural network. You will explore how inputs change through feature extraction in convolutional neural networks (CNNs), and you will interpret the trained filters by the network.\n",
    "\n",
    "#### Dataset\n",
    "\n",
    "You will use the MNIST dataset, a standard dataset of handwritten digits, which is widely used for training and testing image processing systems.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "The code example below demonstrates how to define, train, validate, and test a CNN. The training and test accuracy after each completed epoch are shown after a completed\n",
    "\n",
    "**1. Explore a working example**\n",
    "1. Open `example.ipynb` and read the code.\n",
    "2. Consult the pytorch documentation to learn what the arguments of the various employed pytorch functions mean.\n",
    "3. Run the code.\n",
    "4. Replace SGD with Adam in the training process. Then run the code again.\n",
    "5. Save the output figures that show training and validation accuracy as a function of the number of epochs in your file system.\n",
    "\n",
    "**2. Build a network**\n",
    "Create your own working example. (You are allowed to copy any amount of code from `example.ipynb`.) Your CNN should be different from the CNN in the working example in the following ways:\n",
    "1. The new CNN should have three convolutional layers instead of two. The first layer creates 32 channels. The second layer creates 64 channels, and the third layer creates 128 channels.\n",
    "2. The pooling layer after the third layer should not employ any padding.\n",
    "3. The last hidden layer should have 512 neurons.\n",
    "4. For all layers except the output layer, the activation function should be a ReLU (use `torch.nn.ReLU`).\n",
    "\n",
    "**3. Train and evaluate a neural network**\n",
    "1. Train the neural network that you have constructed in the previous step. How have the upgrades with respect to the CNN in `example.ipynb` affected the CNN's training time?\n",
    "2. Test the neural network. How have the upgrades with respect to the CNN in `example.ipynb` affected the CNN's validation accuracy?\n",
    "3. Identify the number $k$ of training epochs that gives you a good tradeoff between training time and validation accuracy.\n",
    "4. Run your code again using $k$ epochs during training. Time the training (e.g. using the python library `time`).\n",
    "\n",
    "**4. Model validation and model selection**\n",
    "1. Use the validation set approach to identify the best number $c$ of channels in the first convolutional layer (consider $c\\in\\{2,15\\}$).\n",
    "2. Update your neural network architecture so that the first convolutional layer has $c$ channels.\n",
    "\n",
    "**5. Visualizing feature extraction**\n",
    "1. Use the function `plot_mapped_features` to view an input image and the corresponding first channel of the hidden state for each feature-extraction layer (i.e., each convolution layer and each pooling layer).\n",
    "2. Update the function so that it shows all channels instead of just one.\n",
    "3. Comment on where you observe differences between the channels within a layer.\n",
    "\n",
    "**6. Visualizing and interpreting filters**\n",
    "1. Use the function `plot_filters` to view the trained filters of the first convolutional layer.\n",
    "2. Identify the filters that perform blurring, sharpening, or horizontal or vertical edge detection.\n",
    "\n",
    "**7. Comparison to logistic regression**\n",
    "1. Construct and run a pipeline for multiclass logistic regression of the MNIST dataset using sklearn.\n",
    "2. Comment on how the training time and test accuracy of logistic regression compare to the CNN.\n",
    "3. Now run multiclass logistic regression on the MNIST data set using one of the hidden states of the CNN (i.e., $\\vec{x}^{(2)}$, $\\vec{x}^{(3)}$, ..., $\\vec{x}^{(7)}$) as inputs. Which set of inputs yields the best classification results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e15d290-c0b4-4ec9-8db4-908d3c4d1374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5772c922-4df1-4972-b704-f3e3d70982fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize the feature maps produced by different layers for a given image\n",
    "def plot_mapped_features(model, image, layers):\n",
    "    '''Example usage: \n",
    "    \n",
    "    >>> examples = iter(test_loader)\n",
    "    >>> example_data, example_labels = next(examples) # get one batch from test set\n",
    "    >>> example_image = example_data[0]\n",
    "    >>> layers = [model.conv1, model.pool, model.conv2, model.pool]\n",
    "    >>> plot_mapped_features(model, example_image, layers)\n",
    "    \n",
    "    '''\n",
    "    # Add a batch dimension to the image tensor (from (channels, height, width) to (1, channels, height, width))\n",
    "    x = image.unsqueeze(0)\n",
    "    # Create a subplot with 1 row and len(layers) columns\n",
    "    fig, axes = plt.subplots(1, len(layers))\n",
    "    # Iterate over the specified layers\n",
    "    for i, layer in enumerate(layers):\n",
    "        # Pass the image through the current layer\n",
    "        x = layer(x)\n",
    "        # Detach the feature map from the computation graph and move it to CPU, then convert it to a NumPy array\n",
    "        # Visualize the first channel of the feature map\n",
    "        axes[i].imshow(x[0, 0].detach().cpu().numpy(), cmap='gray')\n",
    "        # Turn off the axis for a cleaner look\n",
    "        axes[i].axis('off')\n",
    "    # Display the feature maps\n",
    "    plt.show()\n",
    "    \n",
    "# Function to visualize the filters of a given convolutional layer\n",
    "def plot_filters(layer, n_filters=6):\n",
    "    '''Example usage: \n",
    "\n",
    "    >>> layer = model.conv1\n",
    "    >>> plot_filters(layer, n_filters=6)\n",
    "    \n",
    "    '''\n",
    "    # Clone the weights of the convolutional layer to avoid modifying the original weights\n",
    "    filters = layer.weight.data.clone()\n",
    "    # Normalize the filter values to the range [0, 1] for better visualization\n",
    "    filters = filters - filters.min()\n",
    "    filters = filters / filters.max()\n",
    "    # Select the first n_filters to visualize\n",
    "    filters = filters[:n_filters]\n",
    "    # Create a subplot with 1 row and n_filters columns\n",
    "    fig, axes = plt.subplots(1, n_filters)\n",
    "    # Iterate over the selected filters\n",
    "    for i, filter in enumerate(filters):\n",
    "        # Transpose the filter dimensions to (height, width, channels) for visualization\n",
    "        axes[i].imshow(np.transpose(filter, (1, 2, 0)))\n",
    "        # Turn off the axis for a cleaner look\n",
    "        axes[i].axis('off')\n",
    "    # Display the filters\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
